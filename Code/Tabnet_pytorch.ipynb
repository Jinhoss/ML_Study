{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabnet_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iveo9UU1FoqL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07BHYUjaGH2Z"
      },
      "source": [
        "**고스트 배치 정규화(GBN)**  \n",
        "GBN을 사용하면 대량의 데이터 배치를 훈련하고 동시에 더 잘 일반화 할 수 있다. 입력 배치를 동일한 크기의 하위 배치로 분할하고 동일한 배치 정규화 레이어를 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "994JDi0pF7gX"
      },
      "source": [
        "class GBN(nn.Module):\n",
        "    def __init(self, inp, vbs = 128, momentum = 0.01):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm1d(inp, momentum= momentum)\n",
        "        self.vbs = vbs\n",
        "    def forward(self, x):\n",
        "        chunk = torch.chunk(x, x.size(0) // self.vbs, 0)\n",
        "        res = [self.bn(y) for y in chunk]\n",
        "        return torch.cat(res, 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biDwuIOeHIG_"
      },
      "source": [
        "Sparsemax의 구현\n",
        "https://github.com/gokceneraslan/SparseMax.torch \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMqhzz-jHj4A"
      },
      "source": [
        "**Attention Transformer**  \n",
        "완전히 연결된 계층, GBN laryer 및 Sparsemax 계층으로 구성된다.   \n",
        "Attention transformer는 입력 기능, 이전 단계에서 처리된 기능 및 사용된 기능에 대한 이전 정보를 수신한다. \n",
        "이전 정보는 batch_size x input_features 크기의 행렬로 표시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laeMsqPzG_Vr"
      },
      "source": [
        "class AttentionTransformer(nn.Module):\n",
        "    def __init__(self, d_a, inp_dim, relax, vbs = 128):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_a, inp_dim)\n",
        "        self.bn = GBN(out_dim, vbs = vbs)\n",
        "        self.smax = Sparsmax()\n",
        "        self.r = relax\n",
        "\n",
        "    # feature from previous decision step\n",
        "    def forward(self, a, priors):\n",
        "        a = self.bn(self.fc(a))\n",
        "        mask = self.smax(a * priors)\n",
        "        priors = priors * (self.r-mask) # updating the prior\n",
        "        return mask"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czi_Tm7uIsnh"
      },
      "source": [
        "**Feature Transformer**  \n",
        "선택한 모든 feature들이 처리되어 최종 출력을 생성하는 곳, 여러 개의 게이트 선형 단위 블록으로 구성된다.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZuuVUuaImo9"
      },
      "source": [
        "class GLU(nn.Module):\n",
        "    def __init__(self, inp_dim, out_dim, fc = None, vbs = 128):\n",
        "        super().__init__()\n",
        "        if fc:\n",
        "            self.fc = fc\n",
        "        else:\n",
        "            self.fc = nn.Linear(inp_dim, out_dim * 2)\n",
        "        self.bn = GBN(out_dim * 2, vbs = vbs)\n",
        "        self.od = out_dim\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.bn(self, fc(x))\n",
        "        return x[:, :self.od] * torch.sigmoid(x[:, self.od:])\n",
        "\n",
        "\n",
        "class FeatureTrnasformer(nn.Module):\n",
        "    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs = 128):\n",
        "        super().__init__()\n",
        "        first = True\n",
        "        self.shared = nn.ModuleList()\n",
        "        if shared:\n",
        "            self.shared.append(GLU(inp_dim, out_dim, shared[0], vbs = vbs))\n",
        "            first = False\n",
        "            for fc in shared[1:]:\n",
        "                self.shared.append(GLU(out_dim, out_dim, fc, vbs = vbs))\n",
        "        self.shared = None\n",
        "        self.independ = nn.ModuleList()\n",
        "        if first:\n",
        "            self.independ.append(GLU(inp, out_dim, vbs = vbs))\n",
        "        for x in range(first, n_ind):\n",
        "            self.independ.append(GLU(out_dim, out_dim, vbs = vbs))\n",
        "        self.scale = torch.sqrt(torch.tensor([.5], device = device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.shared:\n",
        "            x = self.shared[0](x)\n",
        "            for glu in self.shared[1:]:\n",
        "                x = torch.add(x, glu(x))\n",
        "                x = x * self.scale\n",
        "        for glu in self.independ:\n",
        "            x = torch.add(x, glu(x))\n",
        "            x = x * self.scale\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gfxL7P_Kyk1"
      },
      "source": [
        "class DecisionStep(nn.Module):\n",
        "    def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax, vbs = 128):\n",
        "        super().__init__()\n",
        "        self.fea_tran = FeatureTransformer(inp_dim, n_d + n_a, shared, n_ind, vbs)\n",
        "        self.atten_tran = AttentionTransformer(n_a, inp_dim, relax, vbs)\n",
        "\n",
        "    def forward(self, x, a, priors):\n",
        "        mask = self.atten_tran(a, priors)\n",
        "        sparse_loss = ((-1) * mask * torch.log(mask + 1e-10)).mean()\n",
        "        x = self.fea_tran(x * mask)\n",
        "        return x, sparse_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cUfGpS1Lb6Q"
      },
      "source": [
        "class Tabnet(nn.Module):\n",
        "    def __init__(self, inp_dim, final_out_dim, n_d = 64, n_a = 64, n_shared = 2, n_ind = 2, n_steps = 5, relax = 1.2, vbs = 128):\n",
        "        super().__init__()\n",
        "        if nshared > 0:\n",
        "            self.shared = nn.ModuleList()\n",
        "            self.shared.append(nn.Linear(inp_dim, 2 * (n_d + n_a)))\n",
        "            for x in range(n_shared - 1):\n",
        "                self.shared.append(nn.Linear(n_d + n_a, 2 * (n_d + n_a)))\n",
        "        else:\n",
        "            self.shared = None\n",
        "        self.first_step = FeatureTrnasformer(inp_dim, n_d + n_a, self.shared, n_ind)\n",
        "        self.steps = nn.ModuleList()\n",
        "        for x  in range(n_steps - 1):\n",
        "            self.steps.append(DecisionStep(inp_dim, n_d, n_a, self.shared, n_ind, relax, vbs))\n",
        "        self.fc = nn.Linear(n_d, final_out_dim)\n",
        "        self.bn = nn.BatchNorm1d(inp_dim)\n",
        "        self.n_d = n_d\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x_a = self.first_step(x)[:, self.n_d:]\n",
        "        sparse_loss = torch.zeros(1).to(x.device)\n",
        "        out = torch.zeros(x.size(0), self.n_d.to(x.device))\n",
        "        priors = torch.ones(x.shape).to(x.device)\n",
        "        for step in self.steps:\n",
        "            s_te, l  = step(x, x_a, priors)\n",
        "            out += F.relu(x_te[:, :self.n_d])\n",
        "            x_a = x_te[:, self.n_d]\n",
        "            sparse_loss += 1\n",
        "        return self.fc(out), sparse_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdORodhee2X1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}